{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8ad2cea1-42cb-47a4-bc6f-205d1de37fcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "e2f4cd07-5b55-4766-87bc-63b6dc5f4017",
   "metadata": {},
   "source": [
    "#Untuk menghapus baris dengan data yang hilang (missing values):\n",
    "df.dropna(inplace=True)\n",
    "#Untuk melakukan imputasi pada nilai yang hilang dengan nilai rata-rata kolom:\n",
    "df.fillna(df.mean(), inplace=True)\n",
    "#Untuk menghapus baris dengan nilai yang tidak valid atau outliers berdasarkan suatu atribut:\n",
    "df = df[(df['atribut'] >= nilai_bawah) & (df['atribut'] <= nilai_atas)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a1e4e60e-05f8-4a3c-b3fc-f18e617ccb5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Membaca dataset dari file CSV menggunakan Pandas\n",
    "df = pd.read_csv(\"heart_cleveland_upload.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4e33431e-2494-4f6f-bff7-e59eaa8205c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>sex</th>\n",
       "      <th>cp</th>\n",
       "      <th>trestbps</th>\n",
       "      <th>chol</th>\n",
       "      <th>fbs</th>\n",
       "      <th>restecg</th>\n",
       "      <th>thalach</th>\n",
       "      <th>exang</th>\n",
       "      <th>oldpeak</th>\n",
       "      <th>slope</th>\n",
       "      <th>ca</th>\n",
       "      <th>thal</th>\n",
       "      <th>condition</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>69</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>160</td>\n",
       "      <td>234</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>131</td>\n",
       "      <td>0</td>\n",
       "      <td>0.1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>69</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>140</td>\n",
       "      <td>239</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>151</td>\n",
       "      <td>0</td>\n",
       "      <td>1.8</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>66</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>150</td>\n",
       "      <td>226</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>114</td>\n",
       "      <td>0</td>\n",
       "      <td>2.6</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>65</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>138</td>\n",
       "      <td>282</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>174</td>\n",
       "      <td>0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>64</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>110</td>\n",
       "      <td>211</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>144</td>\n",
       "      <td>1</td>\n",
       "      <td>1.8</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   age  sex  cp  trestbps  chol  fbs  restecg  thalach  exang  oldpeak  slope  \\\n",
       "0   69    1   0       160   234    1        2      131      0      0.1      1   \n",
       "1   69    0   0       140   239    0        0      151      0      1.8      0   \n",
       "2   66    0   0       150   226    0        0      114      0      2.6      2   \n",
       "3   65    1   0       138   282    1        2      174      0      1.4      1   \n",
       "4   64    1   0       110   211    0        2      144      1      1.8      1   \n",
       "\n",
       "   ca  thal  condition  \n",
       "0   1     0          0  \n",
       "1   2     0          0  \n",
       "2   0     0          0  \n",
       "3   1     0          1  \n",
       "4   0     0          0  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "edeb0509-b580-4266-b23e-1a7501b65e72",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "age            int64\n",
       "sex            int64\n",
       "cp             int64\n",
       "trestbps       int64\n",
       "chol           int64\n",
       "fbs            int64\n",
       "restecg        int64\n",
       "thalach        int64\n",
       "exang          int64\n",
       "oldpeak      float64\n",
       "slope          int64\n",
       "ca             int64\n",
       "thal           int64\n",
       "condition      int64\n",
       "dtype: object"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "64c19e25-c42a-48d9-9af4-03833b2aa2cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(297, 14)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "665b168e-b4bc-4c17-b2c6-7c9e74f8a6ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/296], Loss: 0.9442\n",
      "Epoch [2/296], Loss: 2.3343\n",
      "Epoch [3/296], Loss: 0.4886\n",
      "Epoch [4/296], Loss: 1.7860\n",
      "Epoch [5/296], Loss: 0.4832\n",
      "Epoch [6/296], Loss: 1.6699\n",
      "Epoch [7/296], Loss: 0.4354\n",
      "Epoch [8/296], Loss: 1.1191\n",
      "Epoch [9/296], Loss: 0.5050\n",
      "Epoch [10/296], Loss: 0.8981\n",
      "Epoch [11/296], Loss: 0.5323\n",
      "Epoch [12/296], Loss: 0.8780\n",
      "Epoch [13/296], Loss: 0.5392\n",
      "Epoch [14/296], Loss: 0.8743\n",
      "Epoch [15/296], Loss: 0.5612\n",
      "Epoch [16/296], Loss: 0.8154\n",
      "Epoch [17/296], Loss: 0.5953\n",
      "Epoch [18/296], Loss: 0.7342\n",
      "Epoch [19/296], Loss: 0.6267\n",
      "Epoch [20/296], Loss: 0.6757\n",
      "Epoch [21/296], Loss: 0.6521\n",
      "Epoch [22/296], Loss: 0.6460\n",
      "Epoch [23/296], Loss: 0.6694\n",
      "Epoch [24/296], Loss: 0.6352\n",
      "Epoch [25/296], Loss: 0.6701\n",
      "Epoch [26/296], Loss: 0.6333\n",
      "Epoch [27/296], Loss: 0.6552\n",
      "Epoch [28/296], Loss: 0.6341\n",
      "Epoch [29/296], Loss: 0.6373\n",
      "Epoch [30/296], Loss: 0.6333\n",
      "Epoch [31/296], Loss: 0.6242\n",
      "Epoch [32/296], Loss: 0.6280\n",
      "Epoch [33/296], Loss: 0.6162\n",
      "Epoch [34/296], Loss: 0.6183\n",
      "Epoch [35/296], Loss: 0.6104\n",
      "Epoch [36/296], Loss: 0.6075\n",
      "Epoch [37/296], Loss: 0.6039\n",
      "Epoch [38/296], Loss: 0.5977\n",
      "Epoch [39/296], Loss: 0.5954\n",
      "Epoch [40/296], Loss: 0.5890\n",
      "Epoch [41/296], Loss: 0.5854\n",
      "Epoch [42/296], Loss: 0.5805\n",
      "Epoch [43/296], Loss: 0.5755\n",
      "Epoch [44/296], Loss: 0.5715\n",
      "Epoch [45/296], Loss: 0.5661\n",
      "Epoch [46/296], Loss: 0.5618\n",
      "Epoch [47/296], Loss: 0.5569\n",
      "Epoch [48/296], Loss: 0.5519\n",
      "Epoch [49/296], Loss: 0.5473\n",
      "Epoch [50/296], Loss: 0.5422\n",
      "Epoch [51/296], Loss: 0.5373\n",
      "Epoch [52/296], Loss: 0.5324\n",
      "Epoch [53/296], Loss: 0.5274\n",
      "Epoch [54/296], Loss: 0.5225\n",
      "Epoch [55/296], Loss: 0.5175\n",
      "Epoch [56/296], Loss: 0.5125\n",
      "Epoch [57/296], Loss: 0.5075\n",
      "Epoch [58/296], Loss: 0.5025\n",
      "Epoch [59/296], Loss: 0.4975\n",
      "Epoch [60/296], Loss: 0.4926\n",
      "Epoch [61/296], Loss: 0.4876\n",
      "Epoch [62/296], Loss: 0.4827\n",
      "Epoch [63/296], Loss: 0.4778\n",
      "Epoch [64/296], Loss: 0.4730\n",
      "Epoch [65/296], Loss: 0.4682\n",
      "Epoch [66/296], Loss: 0.4634\n",
      "Epoch [67/296], Loss: 0.4587\n",
      "Epoch [68/296], Loss: 0.4541\n",
      "Epoch [69/296], Loss: 0.4495\n",
      "Epoch [70/296], Loss: 0.4450\n",
      "Epoch [71/296], Loss: 0.4406\n",
      "Epoch [72/296], Loss: 0.4363\n",
      "Epoch [73/296], Loss: 0.4320\n",
      "Epoch [74/296], Loss: 0.4279\n",
      "Epoch [75/296], Loss: 0.4239\n",
      "Epoch [76/296], Loss: 0.4199\n",
      "Epoch [77/296], Loss: 0.4161\n",
      "Epoch [78/296], Loss: 0.4124\n",
      "Epoch [79/296], Loss: 0.4087\n",
      "Epoch [80/296], Loss: 0.4052\n",
      "Epoch [81/296], Loss: 0.4019\n",
      "Epoch [82/296], Loss: 0.3986\n",
      "Epoch [83/296], Loss: 0.3954\n",
      "Epoch [84/296], Loss: 0.3924\n",
      "Epoch [85/296], Loss: 0.3894\n",
      "Epoch [86/296], Loss: 0.3866\n",
      "Epoch [87/296], Loss: 0.3839\n",
      "Epoch [88/296], Loss: 0.3812\n",
      "Epoch [89/296], Loss: 0.3787\n",
      "Epoch [90/296], Loss: 0.3763\n",
      "Epoch [91/296], Loss: 0.3740\n",
      "Epoch [92/296], Loss: 0.3717\n",
      "Epoch [93/296], Loss: 0.3696\n",
      "Epoch [94/296], Loss: 0.3675\n",
      "Epoch [95/296], Loss: 0.3656\n",
      "Epoch [96/296], Loss: 0.3637\n",
      "Epoch [97/296], Loss: 0.3619\n",
      "Epoch [98/296], Loss: 0.3601\n",
      "Epoch [99/296], Loss: 0.3585\n",
      "Epoch [100/296], Loss: 0.3569\n",
      "Epoch [101/296], Loss: 0.3553\n",
      "Epoch [102/296], Loss: 0.3539\n",
      "Epoch [103/296], Loss: 0.3525\n",
      "Epoch [104/296], Loss: 0.3511\n",
      "Epoch [105/296], Loss: 0.3498\n",
      "Epoch [106/296], Loss: 0.3486\n",
      "Epoch [107/296], Loss: 0.3474\n",
      "Epoch [108/296], Loss: 0.3462\n",
      "Epoch [109/296], Loss: 0.3451\n",
      "Epoch [110/296], Loss: 0.3440\n",
      "Epoch [111/296], Loss: 0.3430\n",
      "Epoch [112/296], Loss: 0.3420\n",
      "Epoch [113/296], Loss: 0.3410\n",
      "Epoch [114/296], Loss: 0.3401\n",
      "Epoch [115/296], Loss: 0.3392\n",
      "Epoch [116/296], Loss: 0.3384\n",
      "Epoch [117/296], Loss: 0.3376\n",
      "Epoch [118/296], Loss: 0.3368\n",
      "Epoch [119/296], Loss: 0.3360\n",
      "Epoch [120/296], Loss: 0.3352\n",
      "Epoch [121/296], Loss: 0.3345\n",
      "Epoch [122/296], Loss: 0.3338\n",
      "Epoch [123/296], Loss: 0.3332\n",
      "Epoch [124/296], Loss: 0.3325\n",
      "Epoch [125/296], Loss: 0.3319\n",
      "Epoch [126/296], Loss: 0.3313\n",
      "Epoch [127/296], Loss: 0.3307\n",
      "Epoch [128/296], Loss: 0.3301\n",
      "Epoch [129/296], Loss: 0.3295\n",
      "Epoch [130/296], Loss: 0.3290\n",
      "Epoch [131/296], Loss: 0.3285\n",
      "Epoch [132/296], Loss: 0.3279\n",
      "Epoch [133/296], Loss: 0.3274\n",
      "Epoch [134/296], Loss: 0.3270\n",
      "Epoch [135/296], Loss: 0.3265\n",
      "Epoch [136/296], Loss: 0.3260\n",
      "Epoch [137/296], Loss: 0.3256\n",
      "Epoch [138/296], Loss: 0.3251\n",
      "Epoch [139/296], Loss: 0.3247\n",
      "Epoch [140/296], Loss: 0.3243\n",
      "Epoch [141/296], Loss: 0.3239\n",
      "Epoch [142/296], Loss: 0.3235\n",
      "Epoch [143/296], Loss: 0.3231\n",
      "Epoch [144/296], Loss: 0.3228\n",
      "Epoch [145/296], Loss: 0.3224\n",
      "Epoch [146/296], Loss: 0.3220\n",
      "Epoch [147/296], Loss: 0.3217\n",
      "Epoch [148/296], Loss: 0.3213\n",
      "Epoch [149/296], Loss: 0.3210\n",
      "Epoch [150/296], Loss: 0.3207\n",
      "Epoch [151/296], Loss: 0.3204\n",
      "Epoch [152/296], Loss: 0.3200\n",
      "Epoch [153/296], Loss: 0.3197\n",
      "Epoch [154/296], Loss: 0.3194\n",
      "Epoch [155/296], Loss: 0.3192\n",
      "Epoch [156/296], Loss: 0.3189\n",
      "Epoch [157/296], Loss: 0.3186\n",
      "Epoch [158/296], Loss: 0.3183\n",
      "Epoch [159/296], Loss: 0.3180\n",
      "Epoch [160/296], Loss: 0.3178\n",
      "Epoch [161/296], Loss: 0.3175\n",
      "Epoch [162/296], Loss: 0.3173\n",
      "Epoch [163/296], Loss: 0.3170\n",
      "Epoch [164/296], Loss: 0.3168\n",
      "Epoch [165/296], Loss: 0.3165\n",
      "Epoch [166/296], Loss: 0.3163\n",
      "Epoch [167/296], Loss: 0.3161\n",
      "Epoch [168/296], Loss: 0.3158\n",
      "Epoch [169/296], Loss: 0.3156\n",
      "Epoch [170/296], Loss: 0.3154\n",
      "Epoch [171/296], Loss: 0.3152\n",
      "Epoch [172/296], Loss: 0.3150\n",
      "Epoch [173/296], Loss: 0.3147\n",
      "Epoch [174/296], Loss: 0.3145\n",
      "Epoch [175/296], Loss: 0.3143\n",
      "Epoch [176/296], Loss: 0.3141\n",
      "Epoch [177/296], Loss: 0.3139\n",
      "Epoch [178/296], Loss: 0.3138\n",
      "Epoch [179/296], Loss: 0.3136\n",
      "Epoch [180/296], Loss: 0.3134\n",
      "Epoch [181/296], Loss: 0.3132\n",
      "Epoch [182/296], Loss: 0.3130\n",
      "Epoch [183/296], Loss: 0.3128\n",
      "Epoch [184/296], Loss: 0.3127\n",
      "Epoch [185/296], Loss: 0.3125\n",
      "Epoch [186/296], Loss: 0.3123\n",
      "Epoch [187/296], Loss: 0.3122\n",
      "Epoch [188/296], Loss: 0.3120\n",
      "Epoch [189/296], Loss: 0.3118\n",
      "Epoch [190/296], Loss: 0.3117\n",
      "Epoch [191/296], Loss: 0.3115\n",
      "Epoch [192/296], Loss: 0.3114\n",
      "Epoch [193/296], Loss: 0.3112\n",
      "Epoch [194/296], Loss: 0.3111\n",
      "Epoch [195/296], Loss: 0.3109\n",
      "Epoch [196/296], Loss: 0.3108\n",
      "Epoch [197/296], Loss: 0.3106\n",
      "Epoch [198/296], Loss: 0.3105\n",
      "Epoch [199/296], Loss: 0.3103\n",
      "Epoch [200/296], Loss: 0.3102\n",
      "Epoch [201/296], Loss: 0.3101\n",
      "Epoch [202/296], Loss: 0.3099\n",
      "Epoch [203/296], Loss: 0.3098\n",
      "Epoch [204/296], Loss: 0.3097\n",
      "Epoch [205/296], Loss: 0.3095\n",
      "Epoch [206/296], Loss: 0.3094\n",
      "Epoch [207/296], Loss: 0.3093\n",
      "Epoch [208/296], Loss: 0.3092\n",
      "Epoch [209/296], Loss: 0.3090\n",
      "Epoch [210/296], Loss: 0.3089\n",
      "Epoch [211/296], Loss: 0.3088\n",
      "Epoch [212/296], Loss: 0.3087\n",
      "Epoch [213/296], Loss: 0.3086\n",
      "Epoch [214/296], Loss: 0.3085\n",
      "Epoch [215/296], Loss: 0.3083\n",
      "Epoch [216/296], Loss: 0.3082\n",
      "Epoch [217/296], Loss: 0.3081\n",
      "Epoch [218/296], Loss: 0.3080\n",
      "Epoch [219/296], Loss: 0.3079\n",
      "Epoch [220/296], Loss: 0.3078\n",
      "Epoch [221/296], Loss: 0.3077\n",
      "Epoch [222/296], Loss: 0.3076\n",
      "Epoch [223/296], Loss: 0.3075\n",
      "Epoch [224/296], Loss: 0.3074\n",
      "Epoch [225/296], Loss: 0.3073\n",
      "Epoch [226/296], Loss: 0.3072\n",
      "Epoch [227/296], Loss: 0.3071\n",
      "Epoch [228/296], Loss: 0.3070\n",
      "Epoch [229/296], Loss: 0.3069\n",
      "Epoch [230/296], Loss: 0.3068\n",
      "Epoch [231/296], Loss: 0.3067\n",
      "Epoch [232/296], Loss: 0.3066\n",
      "Epoch [233/296], Loss: 0.3065\n",
      "Epoch [234/296], Loss: 0.3064\n",
      "Epoch [235/296], Loss: 0.3063\n",
      "Epoch [236/296], Loss: 0.3062\n",
      "Epoch [237/296], Loss: 0.3061\n",
      "Epoch [238/296], Loss: 0.3061\n",
      "Epoch [239/296], Loss: 0.3060\n",
      "Epoch [240/296], Loss: 0.3059\n",
      "Epoch [241/296], Loss: 0.3058\n",
      "Epoch [242/296], Loss: 0.3057\n",
      "Epoch [243/296], Loss: 0.3056\n",
      "Epoch [244/296], Loss: 0.3055\n",
      "Epoch [245/296], Loss: 0.3055\n",
      "Epoch [246/296], Loss: 0.3054\n",
      "Epoch [247/296], Loss: 0.3053\n",
      "Epoch [248/296], Loss: 0.3052\n",
      "Epoch [249/296], Loss: 0.3051\n",
      "Epoch [250/296], Loss: 0.3051\n",
      "Epoch [251/296], Loss: 0.3050\n",
      "Epoch [252/296], Loss: 0.3049\n",
      "Epoch [253/296], Loss: 0.3048\n",
      "Epoch [254/296], Loss: 0.3048\n",
      "Epoch [255/296], Loss: 0.3047\n",
      "Epoch [256/296], Loss: 0.3046\n",
      "Epoch [257/296], Loss: 0.3045\n",
      "Epoch [258/296], Loss: 0.3045\n",
      "Epoch [259/296], Loss: 0.3044\n",
      "Epoch [260/296], Loss: 0.3043\n",
      "Epoch [261/296], Loss: 0.3043\n",
      "Epoch [262/296], Loss: 0.3042\n",
      "Epoch [263/296], Loss: 0.3041\n",
      "Epoch [264/296], Loss: 0.3041\n",
      "Epoch [265/296], Loss: 0.3040\n",
      "Epoch [266/296], Loss: 0.3039\n",
      "Epoch [267/296], Loss: 0.3038\n",
      "Epoch [268/296], Loss: 0.3038\n",
      "Epoch [269/296], Loss: 0.3037\n",
      "Epoch [270/296], Loss: 0.3037\n",
      "Epoch [271/296], Loss: 0.3036\n",
      "Epoch [272/296], Loss: 0.3035\n",
      "Epoch [273/296], Loss: 0.3035\n",
      "Epoch [274/296], Loss: 0.3034\n",
      "Epoch [275/296], Loss: 0.3033\n",
      "Epoch [276/296], Loss: 0.3033\n",
      "Epoch [277/296], Loss: 0.3032\n",
      "Epoch [278/296], Loss: 0.3031\n",
      "Epoch [279/296], Loss: 0.3031\n",
      "Epoch [280/296], Loss: 0.3030\n",
      "Epoch [281/296], Loss: 0.3030\n",
      "Epoch [282/296], Loss: 0.3029\n",
      "Epoch [283/296], Loss: 0.3029\n",
      "Epoch [284/296], Loss: 0.3028\n",
      "Epoch [285/296], Loss: 0.3027\n",
      "Epoch [286/296], Loss: 0.3027\n",
      "Epoch [287/296], Loss: 0.3026\n",
      "Epoch [288/296], Loss: 0.3026\n",
      "Epoch [289/296], Loss: 0.3025\n",
      "Epoch [290/296], Loss: 0.3025\n",
      "Epoch [291/296], Loss: 0.3024\n",
      "Epoch [292/296], Loss: 0.3023\n",
      "Epoch [293/296], Loss: 0.3023\n",
      "Epoch [294/296], Loss: 0.3022\n",
      "Epoch [295/296], Loss: 0.3022\n",
      "Epoch [296/296], Loss: 0.3021\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Menentukan perangkat yang akan digunakan (CPU atau CUDA)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Definisikan model\n",
    "class ANN(nn.Module):\n",
    "    def __init__(self, input_size):\n",
    "        super(ANN, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, 128)\n",
    "        self.fc2 = nn.Linear(128, 64)\n",
    "        self.fc3 = nn.Linear(64, 32)\n",
    "        self.fc4 = nn.Linear(32, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.fc3(x)\n",
    "        x = self.fc4(x)\n",
    "        x = self.sigmoid(x)\n",
    "        return x\n",
    "\n",
    "# Membuat instance model\n",
    "input_size = 13  # Sesuaikan dengan jumlah atribut\n",
    "model = ANN(input_size).to(device)\n",
    "\n",
    "# Definisikan loss function dan optimizer\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Konversi data ke Tensor dan pindahkan ke perangkat yang digunakan\n",
    "X = df.drop(\"condition\", axis=1).values\n",
    "y = df[\"condition\"].values\n",
    "\n",
    "X_tensor = torch.tensor(X, dtype=torch.float32).to(device)\n",
    "y_tensor = torch.tensor(y, dtype=torch.float32).unsqueeze(1).to(device)\n",
    "\n",
    "# Melatih model\n",
    "num_epochs = 296\n",
    "batch_size = 64\n",
    "num_samples = X_tensor.shape[0]\n",
    "num_batches = num_samples // batch_size\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for batch in range(num_batches):\n",
    "        start_idx = batch * batch_size\n",
    "        end_idx = (batch + 1) * batch_size\n",
    "        \n",
    "        inputs = X_tensor[start_idx:end_idx]\n",
    "        labels = y_tensor[start_idx:end_idx]\n",
    "        \n",
    "        # Melakukan forward pass\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        # Melakukan backward pass dan pembaruan bobot\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    # Mencetak loss pada setiap epoch\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dc8c9bae-139e-4fe4-8124-8c5a0b934819",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X_test' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Mengkonversi data pengujian ke Tensor dan memindahkannya ke perangkat yang digunakan\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m X_test_tensor \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(\u001b[43mX_test\u001b[49m, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat32)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m      3\u001b[0m y_test_tensor \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(y_test, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat32)\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# Menjadikan model ke mode evaluasi (tidak ada pembaruan bobot)\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'X_test' is not defined"
     ]
    }
   ],
   "source": [
    "# Mengkonversi data pengujian ke Tensor dan memindahkannya ke perangkat yang digunakan\n",
    "X_test_tensor = torch.tensor(X_test, dtype=torch.float32).to(device)\n",
    "y_test_tensor = torch.tensor(y_test, dtype=torch.float32).unsqueeze(1).to(device)\n",
    "\n",
    "# Menjadikan model ke mode evaluasi (tidak ada pembaruan bobot)\n",
    "model.eval()\n",
    "\n",
    "# Melakukan prediksi pada data pengujian\n",
    "with torch.no_grad():\n",
    "    outputs = model(X_test_tensor)\n",
    "    predicted_labels = (outputs >= 0.5).float()  # Menggunakan threshold 0.5 untuk klasifikasi biner\n",
    "\n",
    "# Menghitung metrik evaluasi\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "accuracy = accuracy_score(y_test, predicted_labels.cpu())\n",
    "precision = precision_score(y_test, predicted_labels.cpu())\n",
    "recall = recall_score(y_test, predicted_labels.cpu())\n",
    "f1 = f1_score(y_test, predicted_labels.cpu())\n",
    "\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall: {recall:.4f}\")\n",
    "print(f\"F1-score: {f1:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f680448-d90b-4ff8-a8f5-afa011597144",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'asep1.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6dfd605-4cfb-4a2b-9000-f1accef8f454",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a02670f-67aa-4d2a-8517-97b5df696935",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
